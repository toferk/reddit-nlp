{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook prepares our dataset for model hyperparameter tuning. After splitting data into training and testing sets, we clean our text for count vectorization and TF-IDF transformations. Using GridSearch, we determine which models and hyperparameters will be best for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.feature_extraction.text  import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from nltk import RegexpTokenizer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data and train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/data_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/r/BravoRealHousewives daily OT thread. Today ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Real Housewives of New Jersey S09E07 - Bru...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If we could pool our money and hire MKE to do ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gotta pay for that wedding somehow but holy Fa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RHONJ Season 9 Midseason Trailer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  /r/BravoRealHousewives daily OT thread. Today ...       1\n",
       "1  The Real Housewives of New Jersey S09E07 - Bru...       1\n",
       "2  If we could pool our money and hire MKE to do ...       1\n",
       "3  Gotta pay for that wedding somehow but holy Fa...       1\n",
       "4                  RHONJ Season 9 Midseason Trailer        1"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>His fresh fade has evolved.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3600</th>\n",
       "      <td>Ron Baker The Virginity Taker</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3601</th>\n",
       "      <td>Ahhh the infamous karma whore. Probably hops i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3602</th>\n",
       "      <td>Fuck Pacers have McDermott... RIP raptors</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3603</th>\n",
       "      <td>I swear to God --- That'd be epic af!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target\n",
       "3599                        His fresh fade has evolved.       0\n",
       "3600                     Ron Baker The Virginity Taker        0\n",
       "3601  Ahhh the infamous karma whore. Probably hops i...       0\n",
       "3602          Fuck Pacers have McDermott... RIP raptors       0\n",
       "3603              I swear to God --- That'd be epic af!       0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.514151\n",
       "0    0.485849\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['text']]\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instatiate lemmatizer, tokenizer, list of stop words, and a function to clean our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare CountVectorizer\n",
    "#instatiate lemmatizer and tokenizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "\n",
    "#create set of stopwords from sklearn and add more words\n",
    "stops = set(stopwords.words('english'))\n",
    "more_stops = ['link','xb','amp','r']\n",
    "for w in more_stops:\n",
    "    stops.add(w)\n",
    "\n",
    "#function to clean text\n",
    "def to_words(raw_text):\n",
    "    #remove links\n",
    "    raw_text = re.sub('http\\S+', '', raw_text)\n",
    "    raw_text = re.sub('www\\S+', '', raw_text)\n",
    "    #remove numbers\n",
    "    raw_text = re.sub('\\d+', '', raw_text)\n",
    "    #tokenize\n",
    "    words = tokenizer.tokenize(raw_text.lower())\n",
    "    #remove stop words\n",
    "    meaningful_words = [lemmatizer.lemmatize(w) for w in words if not w in stops]\n",
    "    \n",
    "    return (\" \".join(meaningful_words))\n",
    "#use our to_words function to create a list of texts for our training and testing set\n",
    "\n",
    "# Initialize empty lists to hold the clean texts.\n",
    "clean_train_text = []\n",
    "clean_test_text = []\n",
    "\n",
    "# Append clean texts to list.\n",
    "for text in X_train['text']:\n",
    "    clean_train_text.append(to_words(text))\n",
    "for text in X_test['text']:\n",
    "    clean_test_text.append(to_words(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rhoa official intro new taglines released',\n",
       " 'southern charm lady',\n",
       " 'nikola jokic flex felipe eichenberger denver nugget head strength coach',\n",
       " 'totally',\n",
       " 'housewife moment life chill part maybe used drama thursday last met hw moment long interestingly explained got thinking though hw worthy moment last year']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jokic realize swaggy p team right',\n",
       " 'bravorealhousewives daily ot thread today november daily thread topic discussion',\n",
       " 'nba shoud introduce hypermax contract basically instead measly cap supermax hypermax would team cap mean player could potentially receive million year nba current million cap would team always gonna front office either desperate enough dumb enough shell money player even mentioned sentence max contract',\n",
       " 'told someone attended party like sign outside house establishment arriving guest consent form sometimes non disclosure agreement given guest completed enter party venue',\n",
       " 'umm want emily back know anti tamra']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune models with CountVectorizer dataframe\n",
    "- Logistic Regression, Random Forest, AdaBoost, Gradient Boost, and NaiveBayes Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 3000, ngram_range=(1,3)) \n",
    "\n",
    "train_cv = cv.fit_transform(clean_train_text)\n",
    "test_cv = cv.transform(clean_test_text)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an array\n",
    "train_cv = train_cv.toarray()\n",
    "test_cv = test_cv.toarray()\n",
    "X_train_cv = pd.DataFrame(train_cv,columns=cv.get_feature_names(),index=y_train)\n",
    "X_test_cv = pd.DataFrame(test_cv,columns=cv.get_feature_names(),index=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaron</th>\n",
       "      <th>aaron fox</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abuse</th>\n",
       "      <th>accent</th>\n",
       "      <th>accident</th>\n",
       "      <th>accidentally</th>\n",
       "      <th>...</th>\n",
       "      <th>yet</th>\n",
       "      <th>yolanda</th>\n",
       "      <th>york</th>\n",
       "      <th>york knicks</th>\n",
       "      <th>young</th>\n",
       "      <th>young player</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>zach</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        aaron  aaron fox  ability  able  absolute  absolutely  abuse  accent  \\\n",
       "target                                                                         \n",
       "0           0          0        0     0         0           0      0       0   \n",
       "1           0          0        0     0         0           0      0       0   \n",
       "0           0          0        0     0         0           0      0       0   \n",
       "1           0          0        0     0         0           0      0       0   \n",
       "1           0          0        0     0         0           0      0       0   \n",
       "\n",
       "        accident  accidentally  ...   yet  yolanda  york  york knicks  young  \\\n",
       "target                          ...                                            \n",
       "0              0             0  ...     0        0     0            0      0   \n",
       "1              0             0  ...     0        0     0            0      0   \n",
       "0              0             0  ...     0        0     0            0      0   \n",
       "1              0             0  ...     0        0     0            0      0   \n",
       "1              0             0  ...     0        0     0            0      0   \n",
       "\n",
       "        young player  youtube  yr  zach  zero  \n",
       "target                                         \n",
       "0                  0        0   0     0     0  \n",
       "1                  0        0   0     0     0  \n",
       "0                  0        0   0     0     0  \n",
       "1                  0        0   0     0     0  \n",
       "1                  0        0   0     0     0  \n",
       "\n",
       "[5 rows x 3000 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.9499623777276147\n",
      "test score 0.8692220969560316\n",
      "LogisticRegression(C=0.9, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr_params = {\n",
    "    'penalty': ['l2','l1'],\n",
    "    'C': [.1, .5, .7, .9, .95, .99,1],\n",
    "}\n",
    "\n",
    "lr_gs = GridSearchCV(lr, lr_params, cv=5)\n",
    "lr_gs.fit(X_train_cv,y_train)\n",
    "print('LogisticRegression best score:', lr_gs.best_score_)\n",
    "print('LogisticRegression test score:', lr_gs.score(X_test_cv,y_test))\n",
    "print(lr_gs.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.7840481565086531\n",
      "best score 0.7791572610985703\n",
      "test score 0.7463359639233371\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators' : [50,75,100,125,150],\n",
    "    'max_depth' : [None,5,10,15],\n",
    "}\n",
    "\n",
    "\n",
    "rf_gs = GridSearchCV(rf, rf_params, cv=5)\n",
    "rf_model = rf_gs.fit(X_train_cv,y_train)\n",
    "print('RandomForest best score:', rf_gs.best_score_)\n",
    "print('RandomForest test score:', rf_gs.score(X_test_cv,y_test))\n",
    "print(rf_gs.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.8856282919488337\n",
      "best score 0.8250564334085779\n",
      "test score 0.798196166854566\n",
      "{'base_estimator__max_depth': 3, 'n_estimators': 40}\n"
     ]
    }
   ],
   "source": [
    "ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier())\n",
    "ada_params = {\n",
    "    'base_estimator__max_depth' : [1,2,3],\n",
    "    'n_estimators' : [40,50,60],\n",
    "}\n",
    "ada_gs = GridSearchCV(ada, param_grid=ada_params, cv = 5)\n",
    "\n",
    "ada_gs.fit(X_train_cv,y_train)\n",
    "print('AdaBoost best score:', ada_gs.best_score_)\n",
    "print('AdaBoost test score:', ada_gs.score(X_test_cv,y_test))\n",
    "print(ada_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.9322799097065463\n",
      "best score 0.8506395786305493\n",
      "test score 0.8523111612175873\n",
      "{'base_estimator__max_depth': 3, 'n_estimators': 60}\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "gb_params = {\n",
    "    'max_depth':[2,3,4],\n",
    "    'learning_rate':[.1,.5,.9],\n",
    "    'n_estimators':[90,100,110]\n",
    "}\n",
    "gb_gs = GridSearchCV(gb,param_grid=gb_params, cv=3)\n",
    "gb_gs.fit(X_train_cv,y_train)\n",
    "print('GradientBoost best score:', gb_gs.best_score_)\n",
    "print('GradientBoost test score:', gb_gs.score(X_test_cv,y_test))\n",
    "print(ada_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9025583145221971\n",
      "0.8737316798196166\n",
      "0.8927794064970609\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_cv,y_train)\n",
    "print('NaiveBayes Multinomial train score:',nb.score(X_train_cv,y_train))\n",
    "print('NaiveBayes Multinomial CV score:',cross_val_score(nb,X_train_cv, y_train).mean())\n",
    "print('NaiveBayes Multinomial test score:',nb.score(X_test_cv,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune models with CountVectorizer dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer(analyzer = \"word\",\n",
    "                     tokenizer = None,\n",
    "                     preprocessor = None,\n",
    "                     stop_words = None, \n",
    "                     max_features = 3000, ngram_range=(1,3))\n",
    "\n",
    "train_tv = tv.fit_transform(clean_train_text)\n",
    "test_tv = tv.transform(clean_test_text)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an array\n",
    "train_tv = train_tv.toarray()\n",
    "test_tv = test_tv.toarray()\n",
    "X_train_tv = pd.DataFrame(train_tv,columns=tv.get_feature_names())\n",
    "X_test_tv = pd.DataFrame(test_tv,columns=tv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.9371708051166291\n",
      "test score 0.8782412626832018\n",
      "LogisticRegression(C=0.9, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "lr_params = {\n",
    "    'penalty': ['l2','l1'],\n",
    "    'C': [.1, .5, .7, .9, .95, .99,1],\n",
    "}\n",
    "\n",
    "lr_gs = GridSearchCV(lr, lr_params, cv=5)\n",
    "lr_gs.fit(X_train_tv,y_train)\n",
    "print('LogisticRegression best score:', lr_gs.best_score_(X_train_tv,y_train))\n",
    "print('LogisticRegression test score:', lr_gs.score(X_test_tv,y_test))\n",
    "print(lr_gs.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.7840481565086531\n",
      "best score 0.7746425884123401\n",
      "test score 0.7497181510710259\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=75, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators' : [50,75,100,125,150],\n",
    "    'max_depth' : [None,5,10,15],\n",
    "}\n",
    "\n",
    "\n",
    "rf_gs = GridSearchCV(rf, rf_params, cv=5)\n",
    "rf_gs.fit(X_train_tv,y_train)\n",
    "print('RandomForset best score:', rf_gs.best_score_)\n",
    "print('RandomForest test score:', rf_gs.score(X_test_tv,y_test))\n",
    "print(rf_gs.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.9341610233258089\n",
      "best score 0.8227990970654627\n",
      "test score 0.8060879368658399\n",
      "{'base_estimator__max_depth': 3, 'n_estimators': 60}\n"
     ]
    }
   ],
   "source": [
    "ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier())\n",
    "ada_params = {\n",
    "    'base_estimator__max_depth' : [1,2,3],\n",
    "    'n_estimators' : [40,50,60],\n",
    "}\n",
    "ada_gs = GridSearchCV(ada, param_grid=ada_params, cv = 5)\n",
    "\n",
    "ada_gs.fit(X_train_tv,y_train)\n",
    "print('AdaBoost best score:', ada_gs.best_score_)\n",
    "print('AdaBoost test score:', ada_gs.score(X_test_tv,y_test))\n",
    "print(ada_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.9721595184349134\n",
      "best score 0.8288186606471031\n",
      "test score 0.8297632468996617\n",
      "{'base_estimator__max_depth': 3, 'n_estimators': 60}\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "gb_params = {\n",
    "    'max_depth':[2,3,4],\n",
    "    'learning_rate':[.1,.5,.9],\n",
    "    'n_estimators':[90,100,110]\n",
    "}\n",
    "gb_gs = GridSearchCV(gb,param_grid=gb_params, cv=3)\n",
    "gb_gs.fit(X_train_tv,y_train)\n",
    "print('Gradient Boost best score:', gb_gs.best_score_)\n",
    "print('Gradient Boost test score:', gb_gs.score(X_test_tv,y_test))\n",
    "print(ada_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayes Multinomial train score: 0.9130595634480208\n",
      "NaiveBayes Multinomial CV score: 0.8879046996133703\n",
      "NaiveBayes Multinomial test score: 0.8734739178690344\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_tv,y_train)\n",
    "print('NaiveBayes Multinomial train score:',nb.score(X_train_tv,y_train))\n",
    "print('NaiveBayes Multinomial CV score:',cross_val_score(nb,X_train_tv, y_train).mean())\n",
    "print('NaiveBayes Multinomial test score:',nb.score(X_test_tv,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
