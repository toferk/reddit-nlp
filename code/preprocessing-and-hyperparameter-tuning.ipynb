{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook prepares our dataset for model hyperparameter tuning. After splitting data into training and testing sets, we clean our text for count vectorization and TF-IDF transformations. Using GridSearch, we determine which models and hyperparameters will be best for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text  import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from nltk import RegexpTokenizer, WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data and train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "df = pd.read_csv('../data/data_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/r/BravoRealHousewives daily OT thread. Today ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Real Housewives of New Jersey S09E07 - Bru...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If we could pool our money and hire MKE to do ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gotta pay for that wedding somehow but holy Fa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RHONJ Season 9 Midseason Trailer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  /r/BravoRealHousewives daily OT thread. Today ...       1\n",
       "1  The Real Housewives of New Jersey S09E07 - Bru...       1\n",
       "2  If we could pool our money and hire MKE to do ...       1\n",
       "3  Gotta pay for that wedding somehow but holy Fa...       1\n",
       "4                  RHONJ Season 9 Midseason Trailer        1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>His fresh fade has evolved.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3600</th>\n",
       "      <td>Ron Baker The Virginity Taker</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3601</th>\n",
       "      <td>Ahhh the infamous karma whore. Probably hops i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3602</th>\n",
       "      <td>Fuck Pacers have McDermott... RIP raptors</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3603</th>\n",
       "      <td>I swear to God --- That'd be epic af!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target\n",
       "3599                        His fresh fade has evolved.       0\n",
       "3600                     Ron Baker The Virginity Taker        0\n",
       "3601  Ahhh the infamous karma whore. Probably hops i...       0\n",
       "3602          Fuck Pacers have McDermott... RIP raptors       0\n",
       "3603              I swear to God --- That'd be epic af!       0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.514151\n",
       "0    0.485849\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Our data has a close to 50-50 split of classes. We do not need to worry about oversampling or undersampling. \n",
    " - A baseline score for our classification model is 51%\n",
    " - 1 = BravoRealHousewives\n",
    " - 2 = NBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "X = df[['text']]\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instatiate lemmatizer, tokenizer, list of stop words, and a function to clean our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instatiate lemmatizer, tokenizer, and stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "#create set of stopwords from sklearn and add more words\n",
    "stops = set(stopwords.words('english'))\n",
    "#remove meaningless characters xb, amp, and r\n",
    "more_stops = ['xb','amp','r']\n",
    "for w in more_stops:\n",
    "    stops.add(w)\n",
    "\n",
    "#function to clean text\n",
    "def to_words(raw_text):\n",
    "    #remove links\n",
    "    raw_text = re.sub('http\\S+', '', raw_text)\n",
    "    raw_text = re.sub('www\\S+', '', raw_text)\n",
    "    #remove numbers\n",
    "    raw_text = re.sub('\\d+', '', raw_text)\n",
    "    #tokenize\n",
    "    words = tokenizer.tokenize(raw_text.lower())\n",
    "    #remove stop words and stem/lemmatize\n",
    "    meaningful_words = [p_stemmer.stem(w) for w in words if not w in stops]\n",
    "    \n",
    "    return (\" \".join(meaningful_words))\n",
    "#use our to_words function to create a list of texts for our training and testing set\n",
    "\n",
    "# Initialize empty lists to hold the clean texts.\n",
    "clean_train_text = []\n",
    "clean_test_text = []\n",
    "\n",
    "# Append clean texts to list.\n",
    "for text in X_train['text']:\n",
    "    clean_train_text.append(to_words(text))\n",
    "for text in X_test['text']:\n",
    "    clean_test_text.append(to_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Our text data has been stemmed and removed of stop words, numbers, and hyperlinks. (In the end, we forego lemmatizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rhoa offici intro new taglin releas',\n",
       " 'southern charm ladi',\n",
       " 'nikola jokic flex felip eichenberg denver nugget head strength coach',\n",
       " 'total',\n",
       " 'housew moment life chill part mayb use drama thursday last met hw moment long interestingli explain got think though hw worthi moment last year']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jokic realiz swaggi p team right',\n",
       " 'bravorealhousew daili ot thread today novemb daili thread topic discuss',\n",
       " 'nba shoud introduc hypermax contract basic instead measli cap supermax hypermax would team cap mean player could potenti receiv million year nba current million cap would team alway gonna front offic either desper enough dumb enough shell money player even mention sentenc max contract',\n",
       " 'told someon attend parti like sign outsid hous establish arriv guest consent form sometim non disclosur agreement given guest complet enter parti venu',\n",
       " 'umm want emili back know anti tamra']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune models with CountVectorizer dataframe\n",
    "- GridSearching through hyperparamters for Logistic Regression, Random Forest, AdaBoost, and Gradient Boost classificatoin models.\n",
    "- Cross score validation on Naive Bayes Multinomial classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create CountVectorizer Dataframe for Train and Test\n",
    "cv = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 1500, ngram_range=(1,3)) \n",
    "\n",
    "train_cv = cv.fit_transform(clean_train_text)\n",
    "test_cv = cv.transform(clean_test_text)\n",
    "\n",
    "train_cv = train_cv.toarray()\n",
    "\n",
    "test_cv = test_cv.toarray()\n",
    "\n",
    "X_train_cv = pd.DataFrame(train_cv,columns=cv.get_feature_names(),index=y_train)\n",
    "X_test_cv = pd.DataFrame(test_cv,columns=cv.get_feature_names(),index=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaron</th>\n",
       "      <th>aaron fox</th>\n",
       "      <th>abil</th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>abus</th>\n",
       "      <th>accord</th>\n",
       "      <th>account</th>\n",
       "      <th>accus</th>\n",
       "      <th>act</th>\n",
       "      <th>...</th>\n",
       "      <th>wwhl</th>\n",
       "      <th>ye</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>year old</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtub</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        aaron  aaron fox  abil  abl  absolut  abus  accord  account  accus  \\\n",
       "target                                                                       \n",
       "0           0          0     0    0        0     0       0        0      0   \n",
       "1           0          0     0    0        0     0       0        0      0   \n",
       "0           0          0     0    0        0     0       0        0      0   \n",
       "1           0          0     0    0        0     0       0        0      0   \n",
       "1           0          0     0    0        0     0       0        0      0   \n",
       "\n",
       "        act   ...    wwhl  ye  yeah  year  year old  yesterday  yet  york  \\\n",
       "target        ...                                                           \n",
       "0         0   ...       0   0     0     0         0          0    0     0   \n",
       "1         0   ...       0   0     0     0         0          0    0     0   \n",
       "0         0   ...       0   0     0     1         0          0    0     0   \n",
       "1         0   ...       0   0     0     0         0          0    0     0   \n",
       "1         0   ...       0   0     0     0         0          0    0     0   \n",
       "\n",
       "        young  youtub  \n",
       "target                 \n",
       "0           0       0  \n",
       "1           0       0  \n",
       "0           0       0  \n",
       "1           0       0  \n",
       "1           0       0  \n",
       "\n",
       "[5 rows x 1500 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataframe of features and their appearance count for each observation\n",
    "X_train_cv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.95, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "LogisticRegression best score: 0.85275619681835\n",
      "LogisticRegression train score: 0.9452460229374768\n",
      "LogisticRegression test score: 0.8679245283018868\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr_params = {\n",
    "    'penalty': ['l2','l1'],\n",
    "    'C': [.1, .5, .7, .9, .95, .99,1],\n",
    "}\n",
    "\n",
    "lr_gs = GridSearchCV(lr, lr_params, cv=5)\n",
    "lr_model_cv = lr_gs.fit(X_train_cv,y_train)\n",
    "print(lr_model_cv.best_estimator_)\n",
    "print('LogisticRegression best score:', lr_model_cv.best_score_)\n",
    "print('LogisticRegression train score:', lr_model_cv.score(X_train_cv,y_train))\n",
    "print('LogisticRegression test score:', lr_model_cv.score(X_test_cv,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=125, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "RandomForest best score: 0.8383277839437662\n",
      "RandomForest train score: 0.975212726600074\n",
      "RandomForest test score: 0.8645948945615982\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators' : [100,125,150],\n",
    "}\n",
    "\n",
    "rf_gs = GridSearchCV(rf, rf_params, cv=5)\n",
    "rf_model_cv = rf_gs.fit(X_train_cv,y_train)\n",
    "print(rf_model_cv.best_estimator_)\n",
    "print('RandomForest best score:', rf_model_cv.best_score_)\n",
    "print('RandomForest train score:', rf_model_cv.score(X_train_cv,y_train))\n",
    "print('RandomForest test score:', rf_model_cv.score(X_test_cv,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=60, random_state=None)\n",
      "AdaBoost best score: 0.7909729929707732\n",
      "AdaBoost train score: 0.9041805401405846\n",
      "AdaBoost test score: 0.8157602663706992\n"
     ]
    }
   ],
   "source": [
    "ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier())\n",
    "ada_params = {\n",
    "    'base_estimator__max_depth' : [1,2,3],\n",
    "    'n_estimators' : [40,50,60],\n",
    "}\n",
    "ada_gs = GridSearchCV(ada, param_grid=ada_params, cv = 5)\n",
    "\n",
    "ada_model_cv = ada_gs.fit(X_train_cv,y_train)\n",
    "print(ada_model_cv.best_estimator_)\n",
    "print('AdaBoost best score:', ada_model_cv.best_score_)\n",
    "print('AdaBoost train score:', ada_model_cv.score(X_train_cv,y_train))\n",
    "print('AdaBoost test score:', ada_model_cv.score(X_test_cv,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.5, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "GradientBoost best score: 0.82944876063633\n",
      "GradientBoost train score: 0.903070662227155\n",
      "GradientBoost test score: 0.8290788013318535\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "gb_params = {\n",
    "    'learning_rate':[.25,.5,.75],\n",
    "}\n",
    "gb_gs = GridSearchCV(gb,param_grid=gb_params, cv=3)\n",
    "gb_model_cv = gb_gs.fit(X_train_cv,y_train)\n",
    "print(gb_model_cv.best_estimator_)\n",
    "print('GradientBoost best score:', gb_model_cv.best_score_)\n",
    "print('GradientBoost train score:', gb_model_cv.score(X_train_cv,y_train))\n",
    "print('GradientBoost test score:', gb_model_cv.score(X_test_cv,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayes Multinomial train score: 0.8945615982241953\n",
      "NaiveBayes Multinomial CV score: 0.8845730141894296\n",
      "NaiveBayes Multinomial test score: 0.8612652608213096\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "\n",
    "nb_model_cv = nb.fit(X_train_cv,y_train)\n",
    "print('NaiveBayes Multinomial train score:',nb_model_cv.score(X_train_cv,y_train))\n",
    "print('NaiveBayes Multinomial CV score:',cross_val_score(nb,X_train_cv, y_train).mean())\n",
    "print('NaiveBayes Multinomial test score:',nb_model_cv.score(X_test_cv,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune models with TF-IDF dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create TF-IDF Dataframe for Train and Test\n",
    "tv = TfidfVectorizer(analyzer = \"word\",\n",
    "                     tokenizer = None,\n",
    "                     preprocessor = None,\n",
    "                     stop_words = None, \n",
    "                     max_features = 1500, ngram_range=(1,3))\n",
    "\n",
    "train_tv = tv.fit_transform(clean_train_text)\n",
    "test_tv = tv.transform(clean_test_text)\n",
    "\n",
    "train_tv = train_tv.toarray()\n",
    "test_tv = test_tv.toarray()\n",
    "\n",
    "X_train_tv = pd.DataFrame(train_tv,columns=tv.get_feature_names())\n",
    "X_test_tv = pd.DataFrame(test_tv,columns=tv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.5, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "LogisticRegression best score: 0.8679245283018868\n",
      "LogisticRegression train score: 0.9197188309285979\n",
      "LogisticRegression test score: 0.8756936736958935\n"
     ]
    }
   ],
   "source": [
    "lr_model_tv = lr_gs.fit(X_train_tv,y_train)\n",
    "print(lr_model_tv.best_estimator_)\n",
    "print('LogisticRegression best score:', lr_model_tv.best_score_)\n",
    "print('LogisticRegression train score:', lr_model_tv.score(X_train_tv,y_train))\n",
    "print('LogisticRegression test score:', lr_model_tv.score(X_test_tv,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=125, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "RandomForset best score: 0.8472068072512023\n",
      "RandomForest train score: 0.975212726600074\n",
      "RandomForest test score: 0.8634850166481687\n"
     ]
    }
   ],
   "source": [
    "rf_model_tv = rf_gs.fit(X_train_tv,y_train)\n",
    "print(rf_gs.best_estimator_)\n",
    "print('RandomForset best score:', rf_model_tv.best_score_)\n",
    "print('RandomForest train score:', rf_model_tv.score(X_train_tv,y_train))\n",
    "print('RandomForest test score:', rf_model_tv.score(X_test_tv,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=40, random_state=None)\n",
      "AdaBoost best score: 0.7846836847946725\n",
      "AdaBoost train score: 0.8386977432482426\n",
      "AdaBoost test score: 0.7746947835738068\n"
     ]
    }
   ],
   "source": [
    "ada_model_tv = ada_gs.fit(X_train_tv,y_train)\n",
    "print(ada_model_tv.best_estimator_)\n",
    "print('AdaBoost best score:', ada_model_tv.best_score_)\n",
    "print('AdaBoost train score:', ada_model_tv.score(X_train_tv,y_train))\n",
    "print('AdaBoost test score:', ada_model_tv.score(X_test_tv,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.25, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "Gradient Boost best score: 0.8087310395856456\n",
      "Gradient Boost train score: 0.9089900110987791\n",
      "Gradient Boost test score: 0.8135405105438401\n"
     ]
    }
   ],
   "source": [
    "gb_model_tv = gb_gs.fit(X_train_tv,y_train)\n",
    "print(gb_gs.best_estimator_)\n",
    "print('Gradient Boost best score:', gb_model_tv.best_score_)\n",
    "print('Gradient Boost train score:', gb_model_tv.score(X_train_tv,y_train))\n",
    "print('Gradient Boost test score:', gb_model_tv.score(X_test_tv,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayes Multinomial train score: 0.9137994820569737\n",
      "NaiveBayes Multinomial CV score: 0.8886437988248984\n",
      "NaiveBayes Multinomial test score: 0.881243063263041\n"
     ]
    }
   ],
   "source": [
    "nb_model_tv = nb.fit(X_train_tv,y_train)\n",
    "print('NaiveBayes Multinomial train score:', nb_model_tv.score(X_train_tv,y_train))\n",
    "print('NaiveBayes Multinomial CV score:', cross_val_score(nb,X_train_tv, y_train).mean())\n",
    "print('NaiveBayes Multinomial test score:', nb_model_tv.score(X_test_tv,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In general, our classification models appear to be overfitting on the training data for both CountVectorized features and TF-IDF features. \n",
    "- Multinomial Naive Bayes performs the best with train and cross-validated scores falling close to the test score. \n",
    "  - This might indicate that our features are very dependent of each other, which is not surprising; the features, even in n-grams of 2 or 3, might need more context in order to be classified more accurately. \n",
    "- For our final model, we will use the CountVectorized features and its respective combination of models, as they are scoring better overall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export data and best models\n",
    "models = {'lr_model_cv' : lr_model_cv.best_estimator_,\n",
    "          'rf_model_cv' : rf_model_cv.best_estimator_,\n",
    "          'ada_model_cv': ada_model_cv.best_estimator_,\n",
    "          'gb_model_cv' : gb_model_cv.best_estimator_,\n",
    "          'nb_model_cv' : nb_model_cv,\n",
    "          'lr_model_tv' : lr_model_tv.best_estimator_,\n",
    "          'rf_model_tv' : rf_model_tv.best_estimator_,\n",
    "          'ada_model_tv': ada_model_tv.best_estimator_,\n",
    "          'gb_model_tv' : gb_model_tv.best_estimator_,\n",
    "          'nb_model_tv' : nb_model_tv,\n",
    "          'X_train' : X_train,\n",
    "          'X_train_cv' : X_train_cv,\n",
    "          'X_train_tv' : X_train_tv,\n",
    "          'X_test_cv' : X_test_cv,\n",
    "          'X_test_tv' : X_test_tv,\n",
    "          'y_train' : y_train,      \n",
    "          'y_test' : y_test,      \n",
    "          'clean_train_text' : clean_train_text,\n",
    "          'clean_test_text' :clean_test_text\n",
    "         }\n",
    "\n",
    "pickle.dump(models, open('../data/models.pk', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
